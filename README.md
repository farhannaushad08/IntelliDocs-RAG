# ğŸ¤– IntelliDocs-RAG: Local PDF Chatbot using RAG & Offline LLM

![Demo 1](demo1.png)
![Demo 2](demo2.png)

**IntelliDocs-RAG** is a powerful, privacy-focused chatbot that allows users to chat with their own PDF documents â€” completely offline. It uses a **Retrieval-Augmented Generation (RAG)** pipeline powered by **local LLMs via Ollama** to generate fast and context-aware answers.

Whether you're a student, researcher, or professional, IntelliDocs-RAG gives you instant insights from multiple PDFs â€” without relying on the cloud.

---

## ğŸ§  Real-World Use Case

> **Imagine you're in a remote location with no internet access**. You have several PDF research papers, manuals, or reports â€” and you need quick answers without manually reading through them.
>
> With **IntelliDocs-RAG**, just upload your PDFs and ask:
>
> - â€œSummarize the methodology section of paper 2.â€
> - â€œWhich papers mention 'transformer architecture'?â€
>
> The system responds instantly â€” powered entirely by **offline AI**, keeping your documents private and secure.

---

## ğŸ”’ Why Offline Matters

Most modern AI apps rely on internet-based APIs (OpenAI, Google, etc.), which pose issues like:

- âŒ Internet dependency
- âŒ Data privacy concerns
- âŒ Costly API calls and rate limits

**IntelliDocs-RAG** provides a solution with:

âœ… **Offline LLM via Ollama**  
âœ… **Local document embeddings**  
âœ… **No cloud or third-party dependency**  
âœ… **Fully open-source and customizable**

---

## ğŸ’» Tech Stack Overview

| Component         | Technology                             | Purpose                                  |
|------------------|-----------------------------------------|------------------------------------------|
| **UI**            | Streamlit                               | File upload, chatbot interface           |
| **PDF Reader**    | PyPDF2                                  | Extracts text from PDFs                  |
| **Chunking**      | LangChain CharacterTextSplitter         | Maintains context in small chunks        |
| **Embeddings**    | OllamaEmbeddings (`deepseek-r1:1.5b`)   | Understands document semantics           |
| **Vector DB**     | FAISS                                   | Fast similarity search among documents   |
| **LLM Backend**   | Ollama + Local Model (`deepseek-r1`)    | Generates context-aware answers locally  |
| **Prompt Engine** | LangChain PromptTemplate                | Constructs question-answering pipeline   |

---

## ğŸ—‚ï¸ Project Structure

```
IntelliDocs-RAG/
â”œâ”€â”€ IntelliDocs_RAG.py        # Main Streamlit app
â”œâ”€â”€ htmlTemplates.py          # Custom HTML styling for UI
â”œâ”€â”€ requirements.txt          # Required Python packages
â”œâ”€â”€ demo1.png                 # UI screenshot 1
â”œâ”€â”€ demo2.png                 # UI screenshot 2
â”œâ”€â”€ photo.jpg                 # Optional logo or branding image
```

---

## âš™ï¸ How to Run the App Locally

### 1. Clone the repository
```bash
git clone https://github.com/farhannaushad08/IntelliDocs-RAG.git
cd IntelliDocs-RAG
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Make sure Ollama is installed and running
Install Ollama from: [https://ollama.com](https://ollama.com)

Download the LLM model used:
```bash
ollama pull deepseek-r1:1.5b
```

### 4. Run the Streamlit app
```bash
streamlit run IntelliDocs_RAG.py
```



## ğŸŒŸ Future Improvements

- ğŸ” Support for multi-language PDFs
- ğŸ“š Summarization & citation features
- ğŸ§  Plug-in architecture for other file formats (DOCX, TXT)
- ğŸŒ Optional web deployment with user authentication


---

## ğŸ™Œ Final Note

**IntelliDocs-RAG** is more than just a PDF chatbot â€” it's a step toward democratizing access to intelligent document understanding, even in **offline or resource-constrained environments**. Whether you're a student in a remote area, a researcher working with sensitive data, or a privacy-conscious developer, this tool empowers you with AI â€” on your own terms.




---

## ğŸ“œ License

This project is licensed under the MIT License. See `LICENSE` file for details.

---
